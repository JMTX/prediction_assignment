---
title: "Prediction_assignment"
author: "JMTX"
date: "25 avril 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Synopsis

In this assignment, the goal was to find a robust method to predict which physical activity the user was doing from different measures detected on a smartphone. We tested several models like a decision tree, logistical regression, a combination of these both models but the method which provides the best accuracy (0.99) was the random forest model.

a few comments: 
-  The "out-of-sample accuracy" of the methods were evaluated using a validation set separated from the train set to select the best method. Indeed the "out-of-sample accuracy" seems to me a more suitable way to evaluate methods for a classification task than using "out-of-sample error". 

- All models were fitted using a 4 folds cross validation.


## Cleaning the data
The data for this project come from the Human Activity Recognition (HAR) project: http://groupware.les.inf.puc-rio.br/har.
A training set and a test set can be downloaded on the coursera project page. As I was not sure if I was allowed to share the link, I will not write it here and I will assume that you have downloaded the data in the following section.

Before fitting our models, we need to clean the data because many measures contains NA values or DIV#0 values. This will help us to reduce the number of usefull variables.

```{r }
rm(list=ls())
#go to the project directory, replace "." by your work directory path in the work_directory variable.
work_directory="."
setwd(work_directory)
getwd()

#Load the training data
training <- read.csv("pml-training.csv")

#remove col which have too many NA values
ts_training<-training[ , colSums(is.na(training)) ==0]

#remove the variables which have weird value like div0 or ""
ts_training2<-ts_training[ , colSums((ts_training =="") | (ts_training =="#DIV/0!")) < 5]

#remove some useless variables for classification 
#(like timestamps or window because we wont use temporal information like 
#we could use in model like sequential neural network) 
ts_training3 <- ts_training2[,-c(1:7)]

dim(ts_training3)
```

After cleaning the data, we obtain a dataframe of 19622 observations of 53 variables.

just a comment: As you can see in the code the 7 first variables were removed because they represented some timestamps or windows which represents some temporal information I did not use in the follwing models. Maybe these variable could be interesting if we used some model which take into account this information like sequential neural network.


## Create the training set and validation set
In order to evaluate the different methods it is important to use dataset which were not used during the training part.
That is why I will split the original training data into a training set "train_set" containing 75% of the original training data, and a validation set "validation_set" containing 25% of the original training data. In the following sections the models will be trained using the "train_set" and evaluated using the "validation_set".

```{r, warning = FALSE }
#create a training (75%) and validation set (25%) from the originalm training dataset
library(caret)

in_train <- createDataPartition(y=ts_training3$classe,
                               p=0.75,list=FALSE)
train_set <- ts_training3[in_train,];validation_set<-ts_training3[-in_train,]

#define the cross validation parameter to divide the train_set in 4 folds for cv
fit_control <- trainControl(method="cv", 4)
```
I also define in the code the cross validation parameter to use during the fit of the models.

## Prediction Models

In this part we will define 4 prediction models and test their performance on the validation set. The 4 models will be:
- a decision tree model
- a logistical regression model
- a combination of both previous models
- a random forest model

# the decision tree model
```{r, cache=TRUE }
#Define first model : simple decision tree
model_fit1 <- train(classe ~ ., data=train_set, method="rpart", trControl=fit_control)

#compute the prediction on the train _set just to have an idea of the in-sample accuracy
model_pred_train1 <- predict(model_fit1, train_set)
in_sample_accuracy1 = sum(train_set$classe==model_pred_train1)/length(model_pred_train1)
in_sample_accuracy1

#compute the prediction of this model on the validation set and compute the confusion matrix
model_pred1 <- predict(model_fit1, validation_set)
out_sample_accuracy1 = sum(validation_set$classe==model_pred1)/length(model_pred1)
confusionMatrix(validation_set$classe, model_pred1)

```
This fist model obtain an out-of-sample accuracy of `r out_sample_accuracy1` which is not really good. At least this model, does not overfit the data because the in-sample (`r in_sample_accuracy1`) and out-of-sample (`r out_sample_accuracy1`) accuracy are pretty similar.

# the logistical regression model
```{r, cache=TRUE , message = FALSE, warning = FALSE}
#define second model : multiple logistical regression
model_fit2 <- train(classe ~ ., data=train_set, method="multinom",
                    trControl=fit_control,trace=FALSE)

#compute the prediction on the train _set just to have an idea of the in-sample accuracy
model_pred_train2 <- predict(model_fit2, train_set)
in_sample_accuracy2 = sum(train_set$classe==model_pred_train2)/length(model_pred_train2)
in_sample_accuracy2

#compute the prediction of this model on the validation set and compute the confusion matrix
model_pred2 <- predict(model_fit2, validation_set)
out_sample_accuracy2 = sum(validation_set$classe==model_pred2)/length(model_pred2)
confusionMatrix(validation_set$classe, model_pred2)

```
This second model (logistic regression ) obtain an out-of-sample accuracy of `r out_sample_accuracy2` which is better than the first one but not good enough for our problem. As the first model, the second one does not overfit the training data because the in-sample (`r in_sample_accuracy2`) and out-of-sample (`r out_sample_accuracy2`) accuracy are pretty similar.


# Stacking both precedent models

The previous models did not obtain very good performance, thus it could be interesting to combine both ones to see if this could increase the accuracy.

```{r, cache=TRUE , message = FALSE, warning = FALSE}
#let s try to combine both first models to see if we can increase accuracy----
#1) compute prediction on training data for both model
model_pred_train1 <- predict(model_fit1, train_set);model_pred_train2 <- predict(model_fit2, train_set);

#2) create a dataframe of prediction values 
pred_data <- data.frame(model_pred_train1,model_pred_train2,classe=train_set$classe)

#3) fit the model
model_stack <- train(classe~.,method="multinom",data=pred_data, trControl=fit_control, trace=FALSE)


#compute the prediction on the train_set just to have an idea of the in-sample accuracy
model_pred_train_stack <- predict(model_stack, pred_data)
in_sample_accuracy_stack = sum(train_set$classe==model_pred_train_stack)/length(model_pred_train_stack)
in_sample_accuracy_stack

#4) create a dataframe of prediction on the validation test 
pred_data_val <- data.frame(model_pred_train1= model_pred1,model_pred_train2=model_pred2,classe=validation_set$classe)
pred_stack <- predict(model_stack,pred_data_val)

out_sample_accuracy_stack = sum(validation_set$classe== pred_stack)/length(pred_stack)
confusionMatrix(validation_set$classe, pred_stack)
```
This stacked model (logistic regression + tree decision ) obtain an out-of-sample accuracy of `r out_sample_accuracy_stack` which is just slitly better than the second one but not nearly as good as we need for our problem. One more time, this model does not overfit the training data because the in-sample (`r in_sample_accuracy_stack`) and out-of-sample (`r out_sample_accuracy_stack`) accuracy are pretty similar.
```{r cars}
summary(cars)
```
# The random forest model

Now, it is time to use a more complex model: the random forest which consists into combining several decision trees and take the class which obtain the majority of the vote of the different trees. Let's see the performance of such a model.

```{r, cache=TRUE , message = FALSE, warning = FALSE}
#Define the fourth model and fit it : random forest model
model_fit4 <- train(classe ~ ., data=train_set, method="rf", trControl=fit_control, trace=FALSE)

#compute the prediction on the train_set just to have an idea of the in-sample accuracy
model_pred_train4 <- predict(model_fit4, train_set)
in_sample_accuracy4 = sum(train_set$classe==model_pred_train4)/length(model_pred_train4)
in_sample_accuracy4

#compute the prediction of this model on the validation set and compute the confusion matrix
model_pred4 <- predict(model_fit4, validation_set)
out_sample_accuracy4 = sum(validation_set$classe==model_pred4)/length(model_pred4)
confusionMatrix(validation_set$classe, model_pred4)
```

This random forest model obtain an out-of-sample accuracy of `r out_sample_accuracy4` which is really great and largely better than all the other models that we've tested in the previous sections. Indeed the confusion matrix seems really nice too, nearly a diagonal matrix. As for the other models, this one does not overfit the training data because the in-sample (`r in_sample_accuracy4`) and out-of-sample (`r out_sample_accuracy4`) accuracy are pretty similar.

## Conclusion

We've tested several models to predict activities from the Human Activity project data and the best model was the random forest obtaining an out of sample precision of (`r out_sample_accuracy4`). Thus we advised to use this model for this problem.